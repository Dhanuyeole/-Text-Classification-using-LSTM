{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31c2d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Importing  library \n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "#2.Data Preprocessing\n",
    "\n",
    "#I am using Sentiment-140 from Kaggle. It contains a labels data of 1.6 Million Tweets and I find it a good \n",
    "#amount of data to train our model.\n",
    "\n",
    "df = pd.read_csv('../input/sentiment140/training.1600000.processed.noemoticon.csv',header=None)\n",
    "\n",
    "df.head()\n",
    "\n",
    "#2.1.Rename Column name  \n",
    "\n",
    "df.columns = ['sentiment', 'id', 'date', 'query', 'user_id', 'text']\n",
    "df.head()\n",
    "\n",
    "#2.2.Going to train only on text to classify its sentiment.Discard the rest of the useless columns.\n",
    "\n",
    "df = df.drop(['id', 'date', 'query', 'user_id'], axis=1)\n",
    "\n",
    "lab_to_sentiment = {0:\"Negative\", 4:\"Positive\"}\n",
    "def label_decoder(label):\n",
    "  return lab_to_sentiment[label]\n",
    "df.sentiment = df.sentiment.apply(lambda x: label_decoder(x))\n",
    "df.head()\n",
    "\n",
    "#2.3.Now analyse the dataset by its distribution.\n",
    "\n",
    "val_count = df.sentiment.value_counts()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(val_count.index, val_count.values)\n",
    "plt.title(\"Sentiment Data Distribution\")\n",
    "\n",
    "import random\n",
    "# creates random indexes to choose from dataframe\n",
    "random_idx_list = [random.randint(1,len(df.text)) for i in range(10)] \n",
    "# Returns the rows with the index and display it\n",
    "df.loc[random_idx_list,:].head(10) \n",
    "#Have no value as feature to the model we are training. So we need to get rid of them.\n",
    "\n",
    "#3.Text Preprocessing\n",
    "#NLTK is a python library which got functions to perform text processing task for NLP.\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "\n",
    "def preprocess(text, stem=False):\n",
    "  text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n",
    "  tokens = []\n",
    "  for token in text.split():\n",
    "    if token not in stop_words:\n",
    "      if stem:\n",
    "        tokens.append(stemmer.stem(token))\n",
    "      else:\n",
    "        tokens.append(token)\n",
    "  return \" \".join(tokens)\n",
    "\n",
    "df.text = df.text.apply(lambda x: preprocess(x)) \n",
    "\n",
    "\n",
    "#3.1.Positive Words\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "plt.figure(figsize = (20,20)) \n",
    "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.sentiment == 'Positive'].text))\n",
    "plt.imshow(wc , interpolation = 'bilinear')\n",
    "\n",
    "#3.2.Negative Words\n",
    "\n",
    "plt.figure(figsize = (20,20)) \n",
    "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.sentiment == 'Negative'].text))\n",
    "plt.imshow(wc , interpolation = 'bilinear')\n",
    "\n",
    "#3.3Train and Test Split\n",
    "#Splits Dataset into Training and Testing set\n",
    "\n",
    "TRAIN_SIZE = 0.8\n",
    "MAX_NB_WORDS = 100000\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size=1-TRAIN_SIZE,random_state=7) \n",
    "print(\"Train Data size:\", len(train_data))\n",
    "print(\"Test Data size\", len(test_data))\n",
    "\n",
    "train_data.head(10)\n",
    "\n",
    "#4.Tokenization\n",
    "#tokenization is the task of chopping it up into pieces, called tokens , #perhaps at the same time throwing away certain characters, \n",
    "#such as punctuation. The process is called Tokenization.\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data.text)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Vocabulary Size :\", vocab_size)\n",
    "\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "x_train = pad_sequences(tokenizer.texts_to_sequences(train_data.text),\n",
    "                        maxlen = MAX_SEQUENCE_LENGTH)\n",
    "x_test = pad_sequences(tokenizer.texts_to_sequences(test_data.text),\n",
    "                       maxlen = MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(\"Training X Shape:\",x_train.shape)\n",
    "print(\"Testing X Shape:\",x_test.shape)\n",
    "\n",
    "labels = train_data.sentiment.unique().tolist()\n",
    "\n",
    "#4.1.Label Encoding\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_data.sentiment.to_list())\n",
    "\n",
    "y_train = encoder.transform(train_data.sentiment.to_list())\n",
    "y_test = encoder.transform(test_data.sentiment.to_list())\n",
    "\n",
    "y_train = y_train.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)\n",
    "\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "#5.Word Emdedding\n",
    "\n",
    "GLOVE_EMB = '/kaggle/working/glove.6B.300d.txt'\n",
    "EMBEDDING_DIM = 300\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 10\n",
    "MODEL_PATH = '.../output/kaggle/working/best_model.hdf5'\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "f = open(GLOVE_EMB)\n",
    "for line in f:\n",
    "  values = line.split()\n",
    "  word = value = values[0]\n",
    "  coefs = np.asarray(values[1:], dtype='float32')\n",
    "  embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' %len(embeddings_index))\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "  embedding_vector = embeddings_index.get(word)\n",
    "  if embedding_vector is not None:\n",
    "    embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
    "                                          EMBEDDING_DIM,\n",
    "                                          weights=[embedding_matrix],\n",
    "                                          input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                          trainable=False)\n",
    "\n",
    "\n",
    "#6.Model Training - LSTM\n",
    "\n",
    "from tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Input, Dropout\n",
    "from tensorflow.keras.layers import SpatialDropout1D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedding_sequences = embedding_layer(sequence_input)\n",
    "x = SpatialDropout1D(0.2)(embedding_sequences)\n",
    "x = Conv1D(64, 5, activation='relu')(x)\n",
    "x = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "model = tf.keras.Model(sequence_input, outputs)\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=LR), loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "ReduceLROnPlateau = ReduceLROnPlateau(factor=0.1,\n",
    "                                     min_lr = 0.01,\n",
    "                                     monitor = 'val_loss',\n",
    "                                     verbose = 1)\n",
    "\n",
    "print(\"Training on GPU...\") if tf.test.is_gpu_available() else print(\"Training on CPU...\")\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "                    validation_data=(x_test, y_test), callbacks=[ReduceLROnPlateau])\n",
    "\n",
    "#7.Model Evaluation\n",
    "s, (at, al) = plt.subplots(2,1)\n",
    "at.plot(history.history['accuracy'], c= 'b')\n",
    "at.plot(history.history['val_accuracy'], c='r')\n",
    "at.set_title('model accuracy')\n",
    "at.set_ylabel('accuracy')\n",
    "at.set_xlabel('epoch')\n",
    "at.legend(['LSTM_train', 'LSTM_val'], loc='upper left')\n",
    "\n",
    "al.plot(history.history['loss'], c='m')\n",
    "al.plot(history.history['val_loss'], c='c')\n",
    "al.set_title('model loss')\n",
    "al.set_ylabel('loss')\n",
    "al.set_xlabel('epoch')\n",
    "al.legend(['train', 'val'], loc = 'upper left')\n",
    "\n",
    "def decode_sentiment(score):\n",
    "    return \"Positive\" if score>0.5 else \"Negative\"\n",
    "\n",
    "\n",
    "scores = model.predict(x_test, verbose=1, batch_size=10000)\n",
    "y_pred_1d = [decode_sentiment(score) for score in scores]\n",
    "\n",
    "#7.1Confusion Matrix\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, fontsize=13)\n",
    "    plt.yticks(tick_marks, classes, fontsize=13)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label', fontsize=17)\n",
    "    plt.xlabel('Predicted label', fontsize=17)\n",
    "    \n",
    "cnf_matrix = confusion_matrix(test_data.sentiment.to_list(), y_pred_1d)\n",
    "plt.figure(figsize=(6,6))\n",
    "plot_confusion_matrix(cnf_matrix, classes=test_data.sentiment.unique(), title=\"Confusion matrix\")\n",
    "plt.show()\n",
    "\n",
    "#7.2Classification Scores\n",
    "print(classification_report(list(test_data.sentiment), y_pred_1d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7690485e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc58759",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
